{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"gpuType":"T4","include_colab_link":true,"name":"Macrocircuits","provenance":[],"toc_visible":true},"kernel":{"display_name":"Python 3","language":"python","name":"python3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/Macrocircuits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/main/projects/project-notebooks/Macrocircuits.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>","metadata":{"execution":{}}},{"cell_type":"markdown","source":"# Macrocircuits\n\n***Macrocircuits: Leveraging neural architectural priors and modularity in embodied agents***\n\n**By Neuromatch Academy**\n\n**Content creators:** Divyansha Lachi, Kseniia Shilova  \n\n**Content reviewers:** Eva Dyer, Hannah Choi  \n\n__Production editors:__ Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk\n\n---","metadata":{"execution":{}}},{"cell_type":"markdown","source":"## Background\nThis project explores how we can build a biologically inspired artificial neural network (ANN) architecture, derived from the C. Elegans motor circuit, for the control of a simulated Swimmer agent. Traditional motor control ANNs often rely on generic, fully connected multilayer perceptrons (MLPs), which demand extensive training data, offer limited transferability, and possess complex internal dynamics that challenge interpretability. The project aims to understand how the biologically motivated ANN, which is shaped by evolution to be highly structured and sparse, could help to solve these problems and provide advantages in the domain of motor control. We will train MLPs using algorithms such as PPO, DDPG, and ES, and compare their performance in terms of rewards and sample efficiency with our bio-inspired ANN. The project also includes visualizing the C. Elegans connectome and building the network using this circuitry. We will conduct various ablation analyses by removing sign and weight-sharing constraints, and altering environmental parameters like the swimmerâ€™s length or viscosity. These investigations aim to understand how architecture and modularity impact performance and learning across different environments. Finally, the project aims at building an agent that is robust to environmental variations, navigating towards specific targets, and enhancing our understanding of bio-inspired motor control.  \n\n\n**Relevant references:**  \n\n- [Neural circuit architectural priors for embodied control](https://arxiv.org/abs/2201.05242)  \n- [Hierarchical motor control in mammals and machines](https://www.nature.com/articles/s41467-019-13239-6)  \n- [Continuous control with deep reinforcement learning](https://arxiv.org/pdf/1509.02971.pdf)  \n\n*This notebook uses code from the following GitHub repository:* [ncap](https://github.com/nikhilxb/ncap) by Nikhil X. Bhattasali and Anthony M. Zador and Tatiana A. Engel.\n\n**Infrastructure note:** This notebook contains GPU install guide as well as CPU ones for different OS.","metadata":{"execution":{}}},{"cell_type":"markdown","source":"## Notebook Specific Instructions\n\nThis is made to experiment with different environmental settings here. Delete everything from the old notebook and then add things back piecemeal and modify as desired. Best to have both notebooks open on dual monitors if possible.","metadata":{}},{"cell_type":"markdown","source":"**Tutorial links**\n\nThis particular project connects a couple of distinct ideas explored throughout the course. Firstly, the innate ability to learn a certain set of actions quickly is the main topic of [Tutorial 4](https://neuroai.neuromatch.io/tutorials/W2D4_Macrolearning/student/W2D4_Tutorial4.html) for **W2D4** on biological meta-learning. The focus comes with the observation that the brain is not of a generic architecture but is a highly structured and optimized hierarchy of modules, the importance of which is highlighted in [Tutorial 3](https://neuroai.neuromatch.io/tutorials/W2D1_Macrocircuits/student/W2D1_Tutorial3.html) for **W2D1**, forming inductive bias for efficient motor control. The default model for the agent used here is already known Actor-Critic; you had the opportunity to observe in already mentioned tutorials as well as in [Tutorial 3](https://neuroai.neuromatch.io/tutorials/W1D2_ComparingTasks/student/W1D2_Tutorial3.html) for **W1D2**.","metadata":{"execution":{}}},{"cell_type":"markdown","source":"---\n## Section 0: Initial setup","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Installing Dependencies (Kaggle GPU case, uncomment if you want to use this one)\n\nimport subprocess\n\nsubprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"libgl1-mesa-glx\", \"libosmesa6\"])\nsubprocess.run([\"pip\", \"install\", \"-q\", \"imageio[ffmpeg]\"])\n\nprint('Installing dm_control...')\n!pip install -q dm_control>=1.0.16\n\n%env MUJOCO_GL=osmesa\n\n!echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")\n!pip install -q dm-acme[envs]\n!mkdir output_videos","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-07-23T15:28:23.915145Z","iopub.execute_input":"2024-07-23T15:28:23.916627Z","iopub.status.idle":"2024-07-23T15:29:57.616655Z","shell.execute_reply.started":"2024-07-23T15:28:23.916578Z","shell.execute_reply":"2024-07-23T15:29:57.615277Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Reading package lists...\nBuilding dependency tree...\nReading state information...\nlibgl1-mesa-glx is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\nThe following NEW packages will be installed:\n  libosmesa6\n0 upgraded, 1 newly installed, 0 to remove and 80 not upgraded.\nNeed to get 3054 kB of archives.\nAfter this operation, 13.8 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libosmesa6 amd64 21.2.6-0ubuntu0.1~20.04.2 [3054 kB]\n","output_type":"stream"},{"name":"stderr","text":"dpkg-preconfigure: unable to re-open stdin: No such file or directory\n","output_type":"stream"},{"name":"stdout","text":"Fetched 3054 kB in 0s (6781 kB/s)\nSelecting previously unselected package libosmesa6:amd64.\n(Reading database ... 113807 files and directories currently installed.)\nPreparing to unpack .../libosmesa6_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libosmesa6:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up libosmesa6:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nProcessing triggers for libc-bin (2.31-0ubuntu9.14) ...\nInstalling dm_control...\nenv: MUJOCO_GL=osmesa\nInstalled dm_control 1.0.20\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Imports and Utility Functions**","metadata":{"execution":{}}},{"cell_type":"code","source":"#@title Importing Libraries\nimport numpy as np\nimport collections\nimport argparse\nimport os\nimport yaml\nimport typing as T\nimport imageio\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import HTML\n\nimport dm_control as dm\nimport dm_control.suite.swimmer as swimmer\nfrom dm_control.rl import control\nfrom dm_control.utils import rewards\nfrom dm_control import suite\nfrom dm_control.suite.wrappers import pixels\nfrom dm_control.suite.utils import randomizers # Austin added\n\nfrom acme import wrappers\n\nfrom torch import nn","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-07-23T15:29:57.619003Z","iopub.execute_input":"2024-07-23T15:29:57.619406Z","iopub.status.idle":"2024-07-23T15:30:03.754856Z","shell.execute_reply.started":"2024-07-23T15:29:57.619369Z","shell.execute_reply":"2024-07-23T15:30:03.753682Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#@title Utility code for displaying videos\ndef write_video(\n  filepath: os.PathLike,\n  frames: T.Iterable[np.ndarray],\n  fps: int = 60,\n  macro_block_size: T.Optional[int] = None,\n  quality: int = 10,\n  verbose: bool = False,\n  **kwargs,\n):\n  \"\"\"\n  Saves a sequence of frames as a video file.\n\n  Parameters:\n  - filepath (os.PathLike): Path to save the video file.\n  - frames (Iterable[np.ndarray]): An iterable of frames, where each frame is a numpy array.\n  - fps (int, optional): Frames per second, defaults to 60.\n  - macro_block_size (Optional[int], optional): Macro block size for video encoding, can affect compression efficiency.\n  - quality (int, optional): Quality of the output video, higher values indicate better quality.\n  - verbose (bool, optional): If True, prints the file path where the video is saved.\n  - **kwargs: Additional keyword arguments passed to the imageio.get_writer function.\n\n  Returns:\n  None. The video is written to the specified filepath.\n  \"\"\"\n\n  with imageio.get_writer(filepath,\n                        fps=fps,\n                        macro_block_size=macro_block_size,\n                        quality=quality,\n                        **kwargs) as video:\n    if verbose: print('Saving video to:', filepath)\n    for frame in frames:\n      video.append_data(frame)\n\n\ndef display_video(\n  frames: T.Iterable[np.ndarray],\n  filename='output_videos/temp.mp4',\n  fps=60,\n  **kwargs,\n):\n  \"\"\"\n  Displays a video within a Jupyter Notebook from an iterable of frames.\n\n  Parameters:\n  - frames (Iterable[np.ndarray]): An iterable of frames, where each frame is a numpy array.\n  - filename (str, optional): Temporary filename to save the video before display, defaults to 'output_videos/temp.mp4'.\n  - fps (int, optional): Frames per second for the video display, defaults to 60.\n  - **kwargs: Additional keyword arguments passed to the write_video function.\n\n  Returns:\n  HTML object: An HTML video element that can be displayed in a Jupyter Notebook.\n  \"\"\"\n\n  # Write video to a temporary file.\n  filepath = os.path.abspath(filename)\n  write_video(filepath, frames, fps=fps, verbose=False, **kwargs)\n\n  height, width, _ = frames[0].shape\n  dpi = 70\n  orig_backend = matplotlib.get_backend()\n  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n  matplotlib.use(orig_backend)  # Switch back to the original backend.\n  ax.set_axis_off()\n  ax.set_aspect('equal')\n  ax.set_position([0, 0, 1, 1])\n  im = ax.imshow(frames[0])\n  def update(frame):\n    im.set_data(frame)\n    return [im]\n  interval = 1000/fps\n  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n                                  interval=interval, blit=True, repeat=False)\n  return HTML(anim.to_html5_video())","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-07-23T15:30:03.756302Z","iopub.execute_input":"2024-07-23T15:30:03.756918Z","iopub.status.idle":"2024-07-23T15:30:03.772813Z","shell.execute_reply.started":"2024-07-23T15:30:03.756885Z","shell.execute_reply":"2024-07-23T15:30:03.771533Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"\nIn this notebook we will explore the major components essential for this project.\n\n\n*   **Understanding the DeepMind Control Suite Swimmer Agent:** We will begin by exploring the swimmer agent provided by the DeepMind Control Suite. This section includes a detailed exploration of the agent's API, task customization capabilities, and how to adapt the environment to fit our experimental needs.\n*   **Training Models Using Various Reinforcement Learning Algorithms:** Next, we move on to learn how can we train models for the agents we created. We will be using Tonic_RL library to train our model. We will first train a standard MLP model using the Proximal Policy Optimization (PPO) algorithm.\n\n* **Training the NCAP model:** Finally we will define the NCAP model from [Neural Circuit Architectural Priors for Embodied Control](https://arxiv.org/abs/2201.05242) paper. We will train it using PPO and compare it against the MLP model we trained before.\n\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"I'm putting rendering code here as well. With some modification to pass a specific filename.","metadata":{}},{"cell_type":"code","source":"\"\"\" Renders the current environment state to an image \"\"\"\ndef render(env):\n    return env.physics.render(camera_id=0, width=640, height=480)\n\n\"\"\" Tests a DeepMind control suite environment by executing a series of random actions \"\"\"\ndef test_dm_control(env, filename):\n    env = wrappers.CanonicalSpecWrapper(env, clip=True)\n    env = wrappers.SinglePrecisionWrapper(env)\n\n    spec = env.action_spec()\n    timestep = env.reset()\n    frames = [render(env)]\n\n    for _ in range(180): # changed to 3 second renderings\n        action = np.random.uniform(low=spec.minimum, high=spec.maximum, size=spec.shape)\n        timestep = env.step(action)\n        frames.append(render(env))\n    return display_video(frames, filename)\n\n\n# NOTE - not sure which of the registered worms this will slot the model into. \n# Wouldn't that break it? Seems like the action spaces would be different due to different joints ...\n# Since I'm unfamiliar with this API, my best guess is that it will be the swimmer that was last instantiated with the below API call\n# env = suite.load(task, worm, task_kwargs)\n# Therefore, may need to ensure that the pretrained models are the right ones for whichever worm loaded before rendering.\n\"\"\" Renders a video of a saved model checkpoint with the current environment, saves to output \"\"\"\ndef play_model(path, checkpoint='last',environment='default',seed=None, header=None):\n\n    \"\"\"\n\n    Plays a model within an environment and renders the gameplay to a video.\n\n    Parameters:\n    - path (str): Path to the directory containing the model and checkpoints.\n    - checkpoint (str): Specifies which checkpoint to use ('last', 'first', or a specific ID). 'none' indicates no checkpoint.\n    - environment (str): The environment to use. 'default' uses the environment specified in the configuration file.\n    - seed (int): Optional seed for reproducibility.\n    - header (str): Optional Python code to execute before initializing the model, such as importing libraries.\n    \"\"\"\n\n    if checkpoint == 'none':\n        # Use no checkpoint, the agent is freshly created.\n        checkpoint_path = None\n        tonic.logger.log('Not loading any weights')\n    else:\n        checkpoint_path = os.path.join(path, 'checkpoints')\n        if not os.path.isdir(checkpoint_path):\n            tonic.logger.error(f'{checkpoint_path} is not a directory')\n            checkpoint_path = None\n\n        # List all the checkpoints.\n        checkpoint_ids = []\n        for file in os.listdir(checkpoint_path):\n            if file[:5] == 'step_':\n                checkpoint_id = file.split('.')[0]\n                checkpoint_ids.append(int(checkpoint_id[5:]))\n\n        if checkpoint_ids:\n            if checkpoint == 'last':\n                # Use the last checkpoint.\n                checkpoint_id = max(checkpoint_ids)\n                checkpoint_path = os.path.join(checkpoint_path, f'step_{checkpoint_id}')\n            elif checkpoint == 'first':\n                # Use the first checkpoint.\n                checkpoint_id = min(checkpoint_ids)\n                checkpoint_path = os.path.join(checkpoint_path, f'step_{checkpoint_id}')\n            else:\n                # Use the specified checkpoint.\n                checkpoint_id = int(checkpoint)\n                if checkpoint_id in checkpoint_ids:\n                    checkpoint_path = os.path.join(checkpoint_path, f'step_{checkpoint_id}')\n                else:\n                    tonic.logger.error(f'Checkpoint {checkpoint_id} not found in {checkpoint_path}')\n                    checkpoint_path = None\n        else:\n            tonic.logger.error(f'No checkpoint found in {checkpoint_path}')\n            checkpoint_path = None\n\n    # Load the experiment configuration.\n    arguments_path = os.path.join(path, 'config.yaml')\n    with open(arguments_path, 'r') as config_file:\n        config = yaml.load(config_file, Loader=yaml.FullLoader)\n    config = argparse.Namespace(**config)\n\n    # Run the header first, e.g. to load an ML framework.\n    try:\n        if config.header:\n            exec(config.header)\n        if header:\n            exec(header)\n    except:\n        pass\n\n    # Build the agent.\n    agent = eval(config.agent)\n\n    # Build the environment.\n    if environment == 'default':\n        environment  = tonic.environments.distribute(lambda: eval(config.environment))\n    else:\n        environment  = tonic.environments.distribute(lambda: eval(environment))\n    if seed is not None:\n        environment.seed(seed)\n\n    # Initialize the agent.\n    agent.initialize(\n    observation_space=environment.observation_space,\n    action_space=environment.action_space,\n    seed=seed,\n    )\n\n    # Load the weights of the agent form a checkpoint.\n    if checkpoint_path:\n        agent.load(checkpoint_path)\n\n    steps = 0\n    test_observations = environment.start()\n    frames = [environment.render('rgb_array',camera_id=0, width=640, height=480)[0]]\n    score, length = 0, 0\n\n    while True:\n        # Select an action.\n        actions = agent.test_step(test_observations, steps)\n        assert not np.isnan(actions.sum())\n\n        # Take a step in the environment.\n        test_observations, infos = environment.step(actions)\n        frames.append(environment.render('rgb_array',camera_id=0, width=640, height=480)[0])\n        agent.test_update(**infos, steps=steps)\n\n        score += infos['rewards'][0]\n        length += 1\n\n        if infos['resets'][0]:\n          break\n        \n    # video_path = os.path.join(path, 'video.mp4')\n    model_name = video_path.split('/')[-1]\n    video_path = f'output_videos/{model_name}.mp4'\n\n    print('Reward for the run: ', score)\n    return display_video(frames,video_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:30:03.775278Z","iopub.execute_input":"2024-07-23T15:30:03.775673Z","iopub.status.idle":"2024-07-23T15:30:03.804684Z","shell.execute_reply.started":"2024-07-23T15:30:03.775631Z","shell.execute_reply":"2024-07-23T15:30:03.803400Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"---\n## Section 1: Create a DM Swimmer with a target objective\n\nWe will add a ball to the environment. In order to prevent too much sparsity in the reward signal, we will give positive reward if the worm reaches a certain buffer around the target.\n\nThat being said, this may still be too little reward, especially if the distance needed to travel is far away. In the swimmer task, a reward signal is available at each time step. To get closer to that density, we can have a gradation of reward emanating outwards from the target.","metadata":{"execution":{}}},{"cell_type":"markdown","source":"Some notes about the environment.\n\nActually the default objective is to reach the target, and the rewards given are already smoothed out, just as I thought was necessary above. See the reference here and pay careful attention to their get_reward() func: https://github.com/google-deepmind/dm_control/blob/main/dm_control/suite/swimmer.py\n\nI think that we don't need an additional wrapper class for this, but it may still be helpful if we want to modify the smoothness of the reward signal, etc. I will just have all functions overriden for now and then can change at my leisure.\n\nFor better understanding of MuJoCo, skimming this notebook is pretty helpful: https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/python/tutorial.ipynb#scrollTo=Xqo7pyX-n72M","metadata":{}},{"cell_type":"code","source":"_SWIM_SPEED = 0.1 # We can change later on\n\nclass Swim(swimmer.Swimmer):\n    \"\"\"\n    This provides the task for us to use. Previously they had the reward configured on the swim speed. Here we want as distance to the target.\n    \n    We will need to grab that from the environment somehow.\n    \n    Since we're no longer using the speed as a function of reward, pry can take that out of the constructor here.\n    \"\"\"\n    \n    # def __init__(self, target_distance, desired_speed=_SWIM_SPEED, **kwargs):\n    def __init__(self, desired_speed=_SWIM_SPEED, **kwargs):\n        super().__init__(**kwargs)\n        # self.target_distance = target_distance\n        self._desired_speed = desired_speed\n        \n    def initialize_episode(self, physics):\n        \"\"\"\n        This will set the physics. When it says by episode - not sure why it would be changing physics by episode since a standard RL process\n        may take millions of episodes.\n        \n        Is this overloading the term? Perhaps so. Because if we initialize the target a certain distance away then it will reset every\n        episode ...\n        \"\"\"\n        \n        physics.named.model.mat_rgba['target', 'a'] = 1\n        physics.named.model.mat_rgba['target_default', 'a'] = 1\n        physics.named.model.mat_rgba['target_highlight', 'a'] = 1\n        \n        physics.named.model.geom_pos['target', 'x'] = 0.5\n        \n        physics.model.opt.viscosity = 1\n        \n        # what's the difference between geom_pos and model.light_pos ?\n        # answer - one contains the position of the agent, other the light source for rendering.\n        \n        \n        # beginning copy of their code to change at will\n        # Random joint angles:\n        randomizers.randomize_limited_and_rotational_joints(physics, self.random)\n        # Random target position.\n        close_target = self.random.rand() < .2  # Probability of a close target.\n        target_box = .3 if close_target else 2\n        xpos, ypos = self.random.uniform(-target_box, target_box, size=2)\n        physics.named.model.geom_pos['target', 'x'] = xpos\n        physics.named.model.geom_pos['target', 'y'] = ypos\n        physics.named.model.light_pos['target_light', 'x'] = xpos\n        physics.named.model.light_pos['target_light', 'y'] = ypos\n\n        super().initialize_episode(physics)\n\n    def get_observation(self, physics):\n        \"\"\"\n        Note that in the NeuroMatch provided example, the observation did not include distance to target.\n        This is because they were only doing a swim task - add back in for us since we want the target in here.\n        \"\"\"\n        \n        \"\"\"Returns an observation of joint angles, body velocities and target.\"\"\"\n        obs = collections.OrderedDict()\n        obs['joints'] = physics.joints()\n        obs['to_target'] = physics.nose_to_target()\n        obs['body_velocities'] = physics.body_velocities()\n        return obs\n    \n    def get_reward(self, physics):\n        \"\"\"\n        Note that this is the default logic for reaching target with a smooth reward.\n        \n        If we'd like, here is where we can change the task to be the swim task or to be anything else.\n        \"\"\"\n\n        \"\"\"Returns a smooth reward.\"\"\"\n        target_size = physics.named.model.geom_size['target', 0]\n        return rewards.tolerance(physics.nose_to_target_dist(),\n                                 bounds=(0, target_size),\n                                 margin=5*target_size,\n                                 sigmoid='long_tail')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:30:03.806285Z","iopub.execute_input":"2024-07-23T15:30:03.806657Z","iopub.status.idle":"2024-07-23T15:30:03.828175Z","shell.execute_reply.started":"2024-07-23T15:30:03.806629Z","shell.execute_reply":"2024-07-23T15:30:03.825810Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Defining and registering worms\n\nAfter defining the task, we want to create different worms (or, swimmers) that will interact with it. Register them to the environment with some wrapper / context manager / etc at the top.\n\nI am not familiar with this abstraction so am not sure why this is the case, but only run the below cell once to register them with the environment.","metadata":{}},{"cell_type":"code","source":"@swimmer.SUITE.add()\ndef default_worm(\n  n_links=6,\n  desired_speed=_SWIM_SPEED,\n  time_limit=swimmer._DEFAULT_TIME_LIMIT,\n  random=None,\n  environment_kwargs={},\n):\n  \"\"\"Returns the Swim task for a n-link swimmer.\"\"\"\n  model_string, assets = swimmer.get_model_and_assets(n_links)\n  physics = swimmer.Physics.from_xml_string(model_string, assets=assets)\n  task = Swim(desired_speed=desired_speed, random=random)\n  return control.Environment(\n    physics,\n    task,\n    time_limit=time_limit,\n    control_timestep=swimmer._CONTROL_TIMESTEP,\n    **environment_kwargs,\n  )\n\n@swimmer.SUITE.add()\ndef worm_12_links(\n  n_links=12,\n  desired_speed=_SWIM_SPEED,\n  time_limit=swimmer._DEFAULT_TIME_LIMIT,\n  random=None,\n  environment_kwargs={},\n):\n  \"\"\"Returns the Swim task for a n-link swimmer.\"\"\"\n  model_string, assets = swimmer.get_model_and_assets(n_links)\n  physics = swimmer.Physics.from_xml_string(model_string, assets=assets)\n  task = Swim(desired_speed=desired_speed, random=random)\n  return control.Environment(\n    physics,\n    task,\n    time_limit=time_limit,\n    control_timestep=swimmer._CONTROL_TIMESTEP,\n    **environment_kwargs,\n  )\n\n\n# Add your worms to this list for looping over and rendering them all at once\nworms = ['default_worm', 'worm_12_links']","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:30:03.829913Z","iopub.execute_input":"2024-07-23T15:30:03.830390Z","iopub.status.idle":"2024-07-23T15:30:03.843106Z","shell.execute_reply.started":"2024-07-23T15:30:03.830348Z","shell.execute_reply":"2024-07-23T15:30:03.841844Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Here we render the worms to the output_videos directory. This is where all videos will be rendered, including from some pretrained model checkpoints, etc.","metadata":{}},{"cell_type":"code","source":"# note this uses the same swimmer task for all worms\nfor worm in worms:\n    env = suite.load('swimmer', worm, task_kwargs={'random': 1}) # with seed for controlling the RNG\n    test_dm_control(env, filename=f'output_videos/{worm}_untrained.mp4')","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:30:03.844885Z","iopub.execute_input":"2024-07-23T15:30:03.845281Z","iopub.status.idle":"2024-07-23T15:30:54.811993Z","shell.execute_reply.started":"2024-07-23T15:30:03.845245Z","shell.execute_reply":"2024-07-23T15:30:54.810686Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Load tonic_rl and train models / test them on the environment\n\nNote that tonic is extremely annoying to deal with on your own PC. Because it's not set up as a pip package, you need to download the module's entire git repo to whatever subdirectory you're working in, and I found that this caused many annoying import headaches for me. It seems to work fine on Kaggle though, probably because the notebook itself is in root and / or may have less pip packages to get this confused with - not sure.","metadata":{}},{"cell_type":"code","source":"import contextlib\nimport io\n\nwith contextlib.redirect_stdout(io.StringIO()): #to suppress output\n    \n    # cloning tonic from their repo, not the main one, means that the pretrained models are also included\n    # to use them requires a swimmer agent with the same architecture that they define.\n    !git clone https://github.com/neuromatch/tonic\n    %cd tonic\n    \nfrom tonic.torch import models, normalizers\nimport torch\nimport tonic.torch","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:30:54.814081Z","iopub.execute_input":"2024-07-23T15:30:54.815155Z","iopub.status.idle":"2024-07-23T15:30:58.042578Z","shell.execute_reply.started":"2024-07-23T15:30:54.815101Z","shell.execute_reply":"2024-07-23T15:30:58.041170Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The below function is for training both default MLPs and the NCAP model.","metadata":{}},{"cell_type":"code","source":"def train(\n  header,\n  agent,\n  environment,\n  name = 'test',\n  trainer = 'tonic.Trainer()',\n  before_training = None,\n  after_training = None,\n  parallel = 1,\n  sequential = 1,\n  seed = 0\n):\n  \"\"\"\n  Some additional parameters:\n\n  - before_training: Python code to execute immediately before the training loop commences, suitable for setup actions needed after initialization but prior to training.\n  - after_training: Python code to run once the training loop concludes, ideal for teardown or analytical purposes.\n  - parallel: The count of environments to execute in parallel. Limited to 1 in a Colab notebook, but if additional resources are available, this number can be increased to expedite training.\n  - sequential: The number of sequential steps the environment runs before sending observations back to the agent. This setting is useful for temporal batching. It can be disregarded for this tutorial's purposes.\n  - seed: The experiment's random seed, guaranteeing the reproducibility of the training process.\n\n  \"\"\"\n  # Capture the arguments to save them, e.g. to play with the trained agent.\n  args = dict(locals())\n\n  # Run the header first, e.g. to load an ML framework.\n  if header:\n    exec(header)\n\n  # Build the train and test environments.\n  _environment = environment\n  environment = tonic.environments.distribute(lambda: eval(_environment), parallel, sequential)\n  test_environment = tonic.environments.distribute(lambda: eval(_environment))\n\n\n  # Build the agent.\n  agent = eval(agent)\n  agent.initialize(\n    observation_space=test_environment.observation_space,\n    action_space=test_environment.action_space, seed=seed)\n\n  # Choose a name for the experiment.\n  if hasattr(test_environment, 'name'):\n    environment_name = test_environment.name\n  else:\n    environment_name = test_environment.__class__.__name__\n  if not name:\n    if hasattr(agent, 'name'):\n      name = agent.name\n    else:\n      name = agent.__class__.__name__\n    if parallel != 1 or sequential != 1:\n      name += f'-{parallel}x{sequential}'\n\n  # Initialize the logger to save data to the path environment/name/seed.\n  path = os.path.join('data', 'local', 'experiments', 'tonic', environment_name, name)\n  tonic.logger.initialize(path, script_path=None, config=args)\n\n  # Build the trainer.\n  trainer = eval(trainer)\n  trainer.initialize(\n    agent=agent,\n    environment=environment,\n    test_environment=test_environment,\n  )\n  # Run some code before training.\n  if before_training:\n    exec(before_training)\n\n  # Train.\n  trainer.run()\n\n  # Run some code after training.\n  if after_training:\n    exec(after_training)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:30:58.044540Z","iopub.execute_input":"2024-07-23T15:30:58.045596Z","iopub.status.idle":"2024-07-23T15:30:58.060616Z","shell.execute_reply.started":"2024-07-23T15:30:58.045548Z","shell.execute_reply":"2024-07-23T15:30:58.059172Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Below are some model definitions. I think that since we are switching tasks from pure swimming to reaching some target, we will need to retrain all of these models. We may need to do this each time the task is changed because that will change the rewards substantially of course.","metadata":{}},{"cell_type":"code","source":"def ppo_mlp_model(\n  actor_sizes=(64, 64),\n  actor_activation=torch.nn.Tanh,\n  critic_sizes=(64, 64),\n  critic_activation=torch.nn.Tanh,\n):\n\n  \"\"\"\n  Constructs an ActorCritic model with specified architectures for the actor and critic networks.\n\n  Parameters:\n  - actor_sizes (tuple): Sizes of the layers in the actor MLP.\n  - actor_activation (torch activation): Activation function used in the actor MLP.\n  - critic_sizes (tuple): Sizes of the layers in the critic MLP.\n  - critic_activation (torch activation): Activation function used in the critic MLP.\n\n  Returns:\n  - models.ActorCritic: An ActorCritic model comprising an actor and a critic with MLP torsos,\n    equipped with a Gaussian policy head for the actor and a value head for the critic,\n    along with observation normalization.\n  \"\"\"\n\n  return models.ActorCritic(\n    actor=models.Actor(\n      encoder=models.ObservationEncoder(),\n      torso=models.MLP(actor_sizes, actor_activation),\n      head=models.DetachedScaleGaussianPolicyHead(),\n    ),\n    critic=models.Critic(\n      encoder=models.ObservationEncoder(),\n      torso=models.MLP(critic_sizes, critic_activation),\n      head=models.ValueHead(),\n    ),\n    observation_normalizer=normalizers.MeanStd(),\n  )\n\n\n# environments call should be as follows\n# tonic.environments.ControlSuite(\"{TASK}-{SWIMMER AGENT NAME}\")\n# uncomment for a training run - trial this now\n# train('import tonic.torch',\n#       'tonic.torch.agents.PPO(model=ppo_mlp_model(actor_sizes=(256, 256), critic_sizes=(256,256)))',\n#       'tonic.environments.ControlSuite(\"swimmer-default_worm\")',\n#       name = 'mlp_256',\n#       trainer = 'tonic.Trainer(steps=int(5e5),save_steps=int(1e5))')","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:30:58.065090Z","iopub.execute_input":"2024-07-23T15:30:58.065875Z","iopub.status.idle":"2024-07-23T15:30:58.077499Z","shell.execute_reply.started":"2024-07-23T15:30:58.065823Z","shell.execute_reply":"2024-07-23T15:30:58.076356Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above training finishes properly. Once I test things for real tomorrow or Wednesday leaving this running during the tutorial session and then download all trained models to the github from Zahra.","metadata":{}},{"cell_type":"markdown","source":"The above code works just fine for training a new MLP model with PPO on a new environment with the swimmer aiming for a ball, along with a higher viscosity. Now copy the NCAP implementation.\n\nThe NCAP implementation is where I'll really want to change some things. I think that I can add some MLP components to it that will really help with robustness while still keeping the parameter counts much lower than a full MLP. So this will be more of a neuro symbolic architecture in some ways.\n\nHow to do this - instantiate the original NCAP code and then have another version of it that I will add the MLP to. Then I need to train all three of the models (potentially with different optimization algorithms although I think PPO is more than sufficient for this) and then see how they compare on this new task, along with the varying viscosity.\n\nI think I can add in a residual stream abstraction similar to transformers. That will be a very interesting experiment.","metadata":{}},{"cell_type":"markdown","source":"# NCAP definition","metadata":{}},{"cell_type":"code","source":"# ==================================================================================================\n# Weight constraints.\n\n\ndef excitatory(w, upper=None):\n    return w.clamp(min=0, max=upper)\n\n\ndef inhibitory(w, lower=None):\n    return w.clamp(min=lower, max=0)\n\n\ndef unsigned(w, lower=None, upper=None):\n    return w if lower is None and upper is None else w.clamp(min=lower, max=upper)\n\n\n# ==================================================================================================\n# Activation constraints.\n\n\ndef graded(x):\n    return x.clamp(min=0, max=1)\n\n\n# ==================================================================================================\n# Weight initialization.\n\n\ndef excitatory_uniform(shape=(1,), lower=0., upper=1.):\n    assert lower >= 0\n    return nn.init.uniform_(nn.Parameter(torch.empty(shape)), a=lower, b=upper)\n\n\ndef inhibitory_uniform(shape=(1,), lower=-1., upper=0.):\n    assert upper <= 0\n    return nn.init.uniform_(nn.Parameter(torch.empty(shape)), a=lower, b=upper)\n\n\ndef unsigned_uniform(shape=(1,), lower=-1., upper=1.):\n    return nn.init.uniform_(nn.Parameter(torch.empty(shape)), a=lower, b=upper)\n\n\ndef excitatory_constant(shape=(1,), value=1.):\n    return nn.Parameter(torch.full(shape, value))\n\n\ndef inhibitory_constant(shape=(1,), value=-1.):\n    return nn.Parameter(torch.full(shape, value))\n\n\ndef unsigned_constant(shape=(1,), lower=-1., upper=1., p=0.5):\n    with torch.no_grad():\n        weight = torch.empty(shape).uniform_(0, 1)\n        mask = weight < p\n        weight[mask] = upper\n        weight[~mask] = lower\n        return nn.Parameter(weight)\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:30:58.078784Z","iopub.execute_input":"2024-07-23T15:30:58.079202Z","iopub.status.idle":"2024-07-23T15:30:58.095926Z","shell.execute_reply.started":"2024-07-23T15:30:58.079164Z","shell.execute_reply":"2024-07-23T15:30:58.094674Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class CE_NCAP(nn.Module):\n    \"\"\"C.-elegans-inspired neural circuit architectural prior.\"\"\"\n\n    def __init__(\n            self,\n            n_joints: int,\n            n_turn_joints: int = 1,\n            oscillator_period: int = 60,\n            use_weight_sharing: bool = True,\n            use_weight_constraints: bool = True,\n            use_weight_constant_init: bool = True,\n            include_proprioception: bool = True,\n            include_head_oscillators: bool = True,\n            include_speed_control: bool = False,\n            include_turn_control: bool = False,\n    ):\n        super().__init__()\n        self.n_joints = n_joints\n        self.n_turn_joints = n_turn_joints\n        self.oscillator_period = oscillator_period\n        self.include_proprioception = include_proprioception\n        self.include_head_oscillators = include_head_oscillators\n        self.include_speed_control = include_speed_control\n        self.include_turn_control = include_turn_control\n\n        # Log activity\n        self.connections_log = []\n\n        # Timestep counter (for oscillations).\n        self.timestep = 0\n\n        # Weight sharing switch function.\n        self.ws = lambda nonshared, shared: shared if use_weight_sharing else nonshared\n\n        # Weight constraint and init functions.\n        if use_weight_constraints:\n            self.exc = excitatory\n            self.inh = inhibitory\n            if use_weight_constant_init:\n                exc_param = excitatory_constant\n                inh_param = inhibitory_constant\n            else:\n                exc_param = excitatory_uniform\n                inh_param = inhibitory_uniform\n        else:\n            self.exc = unsigned\n            self.inh = unsigned\n            if use_weight_constant_init:\n                exc_param = inh_param = unsigned_constant\n            else:\n                exc_param = inh_param = unsigned_uniform\n\n        # Learnable parameters.\n        self.params = nn.ParameterDict()\n        if use_weight_sharing:\n            if self.include_proprioception:\n                self.params['bneuron_prop'] = exc_param()\n            if self.include_speed_control:\n                self.params['bneuron_speed'] = inh_param()\n            if self.include_turn_control:\n                self.params['bneuron_turn'] = exc_param()\n            if self.include_head_oscillators:\n                self.params['bneuron_osc'] = exc_param()\n            self.params['muscle_ipsi'] = exc_param()\n            self.params['muscle_contra'] = inh_param()\n        else:\n            for i in range(self.n_joints):\n                if self.include_proprioception and i > 0:\n                    self.params[f'bneuron_d_prop_{i}'] = exc_param()\n                    self.params[f'bneuron_v_prop_{i}'] = exc_param()\n\n                if self.include_speed_control:\n                    self.params[f'bneuron_d_speed_{i}'] = inh_param()\n                    self.params[f'bneuron_v_speed_{i}'] = inh_param()\n\n                if self.include_turn_control and i < self.n_turn_joints:\n                    self.params[f'bneuron_d_turn_{i}'] = exc_param()\n                    self.params[f'bneuron_v_turn_{i}'] = exc_param()\n\n                if self.include_head_oscillators and i == 0:\n                    self.params[f'bneuron_d_osc_{i}'] = exc_param()\n                    self.params[f'bneuron_v_osc_{i}'] = exc_param()\n\n                self.params[f'muscle_d_d_{i}'] = exc_param()\n                self.params[f'muscle_d_v_{i}'] = inh_param()\n                self.params[f'muscle_v_v_{i}'] = exc_param()\n                self.params[f'muscle_v_d_{i}'] = inh_param()\n\n    def reset(self):\n        self.timestep = 0\n\n    def log_activity(self, activity_type, neuron):\n        \"\"\"Logs an active connection between neurons.\"\"\"\n        self.connections_log.append((self.timestep, activity_type, neuron))\n\n    def forward(\n            self,\n            joint_pos,\n            right_control=None,\n            left_control=None,\n            speed_control=None,\n            timesteps=None,\n            log_activity=True,\n            log_file='log.txt'\n    ):\n        \"\"\"Forward pass.\n\n    Args:\n      joint_pos (torch.Tensor): Joint positions in [-1, 1], shape (..., n_joints).\n      right_control (torch.Tensor): Right turn control in [0, 1], shape (..., 1).\n      left_control (torch.Tensor): Left turn control in [0, 1], shape (..., 1).\n      speed_control (torch.Tensor): Speed control in [0, 1], 0 stopped, 1 fastest, shape (..., 1).\n      timesteps (torch.Tensor): Timesteps in [0, max_env_steps], shape (..., 1).\n\n    Returns:\n      (torch.Tensor): Joint torques in [-1, 1], shape (..., n_joints).\n    \"\"\"\n\n        exc = self.exc\n        inh = self.inh\n        ws = self.ws\n\n        # Separate into dorsal and ventral sensor values in [0, 1], shape (..., n_joints).\n        joint_pos_d = joint_pos.clamp(min=0, max=1)\n        joint_pos_v = joint_pos.clamp(min=-1, max=0).neg()\n\n        # Convert speed signal from acceleration into brake.\n        if self.include_speed_control:\n            assert speed_control is not None\n            speed_control = 1 - speed_control.clamp(min=0, max=1)\n\n        joint_torques = []  # [shape (..., 1)]\n        for i in range(self.n_joints):\n            bneuron_d = bneuron_v = torch.zeros_like(joint_pos[..., 0, None])  # shape (..., 1)\n\n            # B-neurons recieve proprioceptive input from previous joint to propagate waves down the body.\n            if self.include_proprioception and i > 0:\n                bneuron_d = bneuron_d + joint_pos_d[\n                    ..., i - 1, None] * exc(self.params[ws(f'bneuron_d_prop_{i}', 'bneuron_prop')])\n                bneuron_v = bneuron_v + joint_pos_v[\n                    ..., i - 1, None] * exc(self.params[ws(f'bneuron_v_prop_{i}', 'bneuron_prop')])\n                self.log_activity('exc', f'bneuron_d_prop_{i}')\n                self.log_activity('exc', f'bneuron_v_prop_{i}')\n\n            # Speed control unit modulates all B-neurons.\n            if self.include_speed_control:\n                bneuron_d = bneuron_d + speed_control * inh(\n                    self.params[ws(f'bneuron_d_speed_{i}', 'bneuron_speed')]\n                )\n                bneuron_v = bneuron_v + speed_control * inh(\n                    self.params[ws(f'bneuron_v_speed_{i}', 'bneuron_speed')]\n                )\n                self.log_activity('inh', f'bneuron_d_speed_{i}')\n                self.log_activity('inh', f'bneuron_v_speed_{i}')\n\n            # Turn control units modulate head B-neurons.\n            if self.include_turn_control and i < self.n_turn_joints:\n                assert right_control is not None\n                assert left_control is not None\n                turn_control_d = right_control.clamp(min=0, max=1)  # shape (..., 1)\n                turn_control_v = left_control.clamp(min=0, max=1)\n                bneuron_d = bneuron_d + turn_control_d * exc(\n                    self.params[ws(f'bneuron_d_turn_{i}', 'bneuron_turn')]\n                )\n                bneuron_v = bneuron_v + turn_control_v * exc(\n                    self.params[ws(f'bneuron_v_turn_{i}', 'bneuron_turn')]\n                )\n                self.log_activity('exc', f'bneuron_d_turn_{i}')\n                self.log_activity('exc', f'bneuron_v_turn_{i}')\n\n            # Oscillator units modulate first B-neurons.\n            if self.include_head_oscillators and i == 0:\n                if timesteps is not None:\n                    phase = timesteps.round().remainder(self.oscillator_period)\n                    mask = phase < self.oscillator_period // 2\n                    oscillator_d = torch.zeros_like(timesteps)  # shape (..., 1)\n                    oscillator_v = torch.zeros_like(timesteps)  # shape (..., 1)\n                    oscillator_d[mask] = 1.\n                    oscillator_v[~mask] = 1.\n                else:\n                    phase = self.timestep % self.oscillator_period  # in [0, oscillator_period)\n                    if phase < self.oscillator_period // 2:\n                        oscillator_d, oscillator_v = 1.0, 0.0\n                    else:\n                        oscillator_d, oscillator_v = 0.0, 1.0\n                bneuron_d = bneuron_d + oscillator_d * exc(\n                    self.params[ws(f'bneuron_d_osc_{i}', 'bneuron_osc')]\n                )\n                bneuron_v = bneuron_v + oscillator_v * exc(\n                    self.params[ws(f'bneuron_v_osc_{i}', 'bneuron_osc')]\n                )\n\n                self.log_activity('exc', f'bneuron_d_osc_{i}')\n                self.log_activity('exc', f'bneuron_v_osc_{i}')\n\n            # B-neuron activation.\n            bneuron_d = graded(bneuron_d)\n            bneuron_v = graded(bneuron_v)\n\n            # Muscles receive excitatory ipsilateral and inhibitory contralateral input.\n            muscle_d = graded(\n                bneuron_d * exc(self.params[ws(f'muscle_d_d_{i}', 'muscle_ipsi')]) +\n                bneuron_v * inh(self.params[ws(f'muscle_d_v_{i}', 'muscle_contra')])\n            )\n            muscle_v = graded(\n                bneuron_v * exc(self.params[ws(f'muscle_v_v_{i}', 'muscle_ipsi')]) +\n                bneuron_d * inh(self.params[ws(f'muscle_v_d_{i}', 'muscle_contra')])\n            )\n\n            # Joint torque from antagonistic contraction of dorsal and ventral muscles.\n            joint_torque = muscle_d - muscle_v\n            joint_torques.append(joint_torque)\n\n        self.timestep += 1\n\n        out = torch.cat(joint_torques, -1)  # shape (..., n_joints)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:30:58.097566Z","iopub.execute_input":"2024-07-23T15:30:58.098025Z","iopub.status.idle":"2024-07-23T15:30:58.141077Z","shell.execute_reply.started":"2024-07-23T15:30:58.097985Z","shell.execute_reply":"2024-07-23T15:30:58.139811Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class CE_NCAP_RS(nn.Module):\n    \"\"\"\n    C.-elegans-inspired neural circuit architectural prior.\n    \n    This is Austin's modification to attempt to add in some type of residual stream abstraction.\n    \"\"\"\n\n    def __init__(\n            self,\n            n_joints: int,\n            n_turn_joints: int = 1,\n            oscillator_period: int = 60,\n            use_weight_sharing: bool = True,\n            use_weight_constraints: bool = True,\n            use_weight_constant_init: bool = True,\n            include_proprioception: bool = True,\n            include_head_oscillators: bool = True,\n            include_speed_control: bool = False,\n            include_turn_control: bool = False,\n            verbose: bool = False\n    ):\n        super().__init__()\n        self.n_joints = n_joints\n        self.n_turn_joints = n_turn_joints\n        self.oscillator_period = oscillator_period\n        self.include_proprioception = include_proprioception\n        self.include_head_oscillators = include_head_oscillators\n        self.include_speed_control = include_speed_control\n        self.include_turn_control = include_turn_control\n        self.verbose = verbose\n\n        # Log activity\n        self.connections_log = []\n\n        # Timestep counter (for oscillations).\n        self.timestep = 0\n\n        # Weight sharing switch function.\n        self.ws = lambda nonshared, shared: shared if use_weight_sharing else nonshared\n\n        # Weight constraint and init functions.\n        if use_weight_constraints:\n            self.exc = excitatory\n            self.inh = inhibitory\n            if use_weight_constant_init:\n                exc_param = excitatory_constant\n                inh_param = inhibitory_constant\n            else:\n                exc_param = excitatory_uniform\n                inh_param = inhibitory_uniform\n        else:\n            self.exc = unsigned\n            self.inh = unsigned\n            if use_weight_constant_init:\n                exc_param = inh_param = unsigned_constant\n            else:\n                exc_param = inh_param = unsigned_uniform\n\n        # Learnable parameters.\n        self.params = nn.ParameterDict()\n        if use_weight_sharing:\n            \n            # Austin note - no need to add mem matrix here yet since interested in dorsal v lateral currently\n            \n            if self.include_proprioception:\n                self.params['bneuron_prop'] = exc_param()\n            if self.include_speed_control:\n                self.params['bneuron_speed'] = inh_param()\n            if self.include_turn_control:\n                self.params['bneuron_turn'] = exc_param()\n            if self.include_head_oscillators:\n                self.params['bneuron_osc'] = exc_param()\n            self.params['muscle_ipsi'] = exc_param()\n            self.params['muscle_contra'] = inh_param()\n        else:\n            for i in range(self.n_joints):\n                if self.include_proprioception and i > 0:\n                    self.params[f'bneuron_d_prop_{i}'] = exc_param()\n                    self.params[f'bneuron_v_prop_{i}'] = exc_param()\n\n                if self.include_speed_control:\n                    self.params[f'bneuron_d_speed_{i}'] = inh_param()\n                    self.params[f'bneuron_v_speed_{i}'] = inh_param()\n\n                if self.include_turn_control and i < self.n_turn_joints:\n                    self.params[f'bneuron_d_turn_{i}'] = exc_param()\n                    self.params[f'bneuron_v_turn_{i}'] = exc_param()\n\n                if self.include_head_oscillators and i == 0:\n                    self.params[f'bneuron_d_osc_{i}'] = exc_param()\n                    self.params[f'bneuron_v_osc_{i}'] = exc_param()\n                    \n                self.params[f'muscle_d_d_{i}'] = exc_param()\n                self.params[f'muscle_d_v_{i}'] = inh_param()\n                self.params[f'muscle_v_v_{i}'] = exc_param()\n                self.params[f'muscle_v_d_{i}'] = inh_param()\n            \n            \"\"\"\n            Austin addition\n            \n            Can make this link dependent later. If I have it shared between all links then it could also act as some type of \n            salient filtering mechanism. But if we just pass the values individually, then not really high dimensional vectors\n            that would need filtering ....\n            \"\"\"\n\n    def reset(self):\n        self.timestep = 0\n\n    def log_activity(self, activity_type, neuron):\n        \"\"\"Logs an active connection between neurons.\"\"\"\n        self.connections_log.append((self.timestep, activity_type, neuron))\n\n    def forward(\n            self,\n            joint_pos,\n            right_control=None,\n            left_control=None,\n            speed_control=None,\n            timesteps=None,\n            log_activity=True,\n            log_file='log.txt'\n    ):\n        \"\"\"Forward pass.\n\n    Args:\n      joint_pos (torch.Tensor): Joint positions in [-1, 1], shape (..., n_joints).\n      right_control (torch.Tensor): Right turn control in [0, 1], shape (..., 1).\n      left_control (torch.Tensor): Left turn control in [0, 1], shape (..., 1).\n      speed_control (torch.Tensor): Speed control in [0, 1], 0 stopped, 1 fastest, shape (..., 1).\n      timesteps (torch.Tensor): Timesteps in [0, max_env_steps], shape (..., 1).\n\n    Returns:\n      (torch.Tensor): Joint torques in [-1, 1], shape (..., n_joints).\n    \"\"\"\n\n        exc = self.exc\n        inh = self.inh\n        ws = self.ws\n\n        # Separate into dorsal and ventral sensor values in [0, 1], shape (..., n_joints).\n        joint_pos_d = joint_pos.clamp(min=0, max=1)\n        joint_pos_v = joint_pos.clamp(min=-1, max=0).neg()\n\n        # Convert speed signal from acceleration into brake.\n        if self.include_speed_control:\n            assert speed_control is not None\n            speed_control = 1 - speed_control.clamp(min=0, max=1)\n\n        joint_torques = []  # [shape (..., 1)]\n        for i in range(self.n_joints):\n            bneuron_d = bneuron_v = torch.zeros_like(joint_pos[..., 0, None])  # shape (..., 1)\n            \n            # B-neurons recieve proprioceptive input from previous joint to propagate waves down the body.\n            if self.include_proprioception and i > 0:\n                bneuron_d = bneuron_d + joint_pos_d[\n                    ..., i - 1, None] * exc(self.params[ws(f'bneuron_d_prop_{i}', 'bneuron_prop')])\n                bneuron_v = bneuron_v + joint_pos_v[\n                    ..., i - 1, None] * exc(self.params[ws(f'bneuron_v_prop_{i}', 'bneuron_prop')])\n                self.log_activity('exc', f'bneuron_d_prop_{i}')\n                self.log_activity('exc', f'bneuron_v_prop_{i}')\n\n            # Speed control unit modulates all B-neurons.\n            if self.include_speed_control:\n                bneuron_d = bneuron_d + speed_control * inh(\n                    self.params[ws(f'bneuron_d_speed_{i}', 'bneuron_speed')]\n                )\n                bneuron_v = bneuron_v + speed_control * inh(\n                    self.params[ws(f'bneuron_v_speed_{i}', 'bneuron_speed')]\n                )\n                self.log_activity('inh', f'bneuron_d_speed_{i}')\n                self.log_activity('inh', f'bneuron_v_speed_{i}')\n\n            # Turn control units modulate head B-neurons.\n            if self.include_turn_control and i < self.n_turn_joints:\n                assert right_control is not None\n                assert left_control is not None\n                turn_control_d = right_control.clamp(min=0, max=1)  # shape (..., 1)\n                turn_control_v = left_control.clamp(min=0, max=1)\n                bneuron_d = bneuron_d + turn_control_d * exc(\n                    self.params[ws(f'bneuron_d_turn_{i}', 'bneuron_turn')]\n                )\n                bneuron_v = bneuron_v + turn_control_v * exc(\n                    self.params[ws(f'bneuron_v_turn_{i}', 'bneuron_turn')]\n                )\n                self.log_activity('exc', f'bneuron_d_turn_{i}')\n                self.log_activity('exc', f'bneuron_v_turn_{i}')\n\n            # Oscillator units modulate first B-neurons.\n            if self.include_head_oscillators and i == 0:\n                if timesteps is not None:\n                    phase = timesteps.round().remainder(self.oscillator_period)\n                    mask = phase < self.oscillator_period // 2\n                    oscillator_d = torch.zeros_like(timesteps)  # shape (..., 1)\n                    oscillator_v = torch.zeros_like(timesteps)  # shape (..., 1)\n                    oscillator_d[mask] = 1.\n                    oscillator_v[~mask] = 1.\n                else:\n                    phase = self.timestep % self.oscillator_period  # in [0, oscillator_period)\n                    if phase < self.oscillator_period // 2:\n                        oscillator_d, oscillator_v = 1.0, 0.0\n                    else:\n                        oscillator_d, oscillator_v = 0.0, 1.0\n                bneuron_d = bneuron_d + oscillator_d * exc(\n                    self.params[ws(f'bneuron_d_osc_{i}', 'bneuron_osc')]\n                )\n                bneuron_v = bneuron_v + oscillator_v * exc(\n                    self.params[ws(f'bneuron_v_osc_{i}', 'bneuron_osc')]\n                )\n\n                self.log_activity('exc', f'bneuron_d_osc_{i}')\n                self.log_activity('exc', f'bneuron_v_osc_{i}')\n\n            # B-neuron activation.\n            bneuron_d = graded(bneuron_d)\n            bneuron_v = graded(bneuron_v)\n            \n            \n            \"\"\"\n            Austin addition\n            Can read and write to a shared matrix here. That matrix should not be learnable, but take care to initialize well.\n            \n            \n            Has anyone been able to get the ball just to the right or just to the left of the agent?\n            Then I can have a separate memory matrix for dorsal and lateral streams, use RSA for analysis.\n            \n            Shape of the bneurons: \n            bneuron_d = bneuron_v = torch.zeros_like(joint_pos[..., 0, None])  # shape (..., 1)\n\n            \"\"\"\n            \n            \n            \n            \n\n            # Muscles receive excitatory ipsilateral and inhibitory contralateral input.\n            muscle_d = graded(\n                bneuron_d * exc(self.params[ws(f'muscle_d_d_{i}', 'muscle_ipsi')]) +\n                bneuron_v * inh(self.params[ws(f'muscle_d_v_{i}', 'muscle_contra')])\n            )\n            muscle_v = graded(\n                bneuron_v * exc(self.params[ws(f'muscle_v_v_{i}', 'muscle_ipsi')]) +\n                bneuron_d * inh(self.params[ws(f'muscle_v_d_{i}', 'muscle_contra')])\n            )\n\n            \n            if self.verbose:\n                print(f\"Iteration {i} bneuron_d.shape: \", bneuron_d.shape)\n                print(f\"Iteration {i} muscle_d.shape: \", muscle_d.shape)\n                \n                # below just the same - but does that make sense here?\n                print(f\"Iteration {i} bneuron_d value: \", bneuron_d)\n                print(f\"Iteration {i} muscle_d value:\", muscle_d)\n                \n                # TODO tomorrow check the params dict. But otherwise the read / write may need to be further up ...\n                \n            \n            # Joint torque from antagonistic contraction of dorsal and ventral muscles.\n            joint_torque = muscle_d - muscle_v\n            joint_torques.append(joint_torque)\n\n        self.timestep += 1\n\n        out = torch.cat(joint_torques, -1)  # shape (..., n_joints)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:43:05.155423Z","iopub.execute_input":"2024-07-23T15:43:05.155919Z","iopub.status.idle":"2024-07-23T15:43:05.638270Z","shell.execute_reply.started":"2024-07-23T15:43:05.155879Z","shell.execute_reply":"2024-07-23T15:43:05.636982Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCE_NCAP_RS\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    C.-elegans-inspired neural circuit architectural prior.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    This is Austin's modification to attempt to add in some type of residual stream abstraction.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     10\u001b[0m             n_joints: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m             verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     ):\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"],"ename":"NameError","evalue":"name 'nn' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"I want to step through the above and make sure all the shapes are correct. Typically I would do this in the debugger but Kaggle does not offer that unfortunately. Going to render an environment with this and do a single iteration of training maybe, printing shapes along the way.\n\nAdd a verbose flag to my modded ncap model and then train with one iteration of that. Also need to add the wrapper class for use in tonic.","metadata":{}},{"cell_type":"code","source":"class SwimmerActor(nn.Module):\n    def __init__(\n            self,\n            swimmer,\n            controller=None,\n            distribution=None,\n            timestep_transform=(-1, 1, 0, 1000),\n    ):\n        super().__init__()\n        self.swimmer = swimmer\n        self.controller = controller\n        self.distribution = distribution\n        self.timestep_transform = timestep_transform\n\n    def initialize(\n            self,\n            observation_space,\n            action_space,\n            observation_normalizer=None,\n    ):\n        self.action_size = action_space.shape[0]\n\n    def forward(self, observations):\n        joint_pos = observations[..., :self.action_size]\n        timesteps = observations[..., -1, None]\n\n        # Normalize joint positions by max joint angle (in radians).\n        joint_limit = 2 * np.pi / (self.action_size + 1)  # In dm_control, calculated with n_bodies.\n        joint_pos = torch.clamp(joint_pos / joint_limit, min=-1, max=1)\n\n        # Convert normalized time signal into timestep.\n        if self.timestep_transform:\n            low_in, high_in, low_out, high_out = self.timestep_transform\n            timesteps = (timesteps - low_in) / (high_in - low_in) * (high_out - low_out) + low_out\n\n        # Generate high-level control signals.\n        if self.controller:\n            right, left, speed = self.controller(observations)\n        else:\n            right, left, speed = None, None, None\n\n        # Generate low-level action signals.\n        actions = self.swimmer(\n            joint_pos,\n            timesteps=timesteps,\n            right_control=right,\n            left_control=left,\n            speed_control=speed,\n        )\n\n        # Pass through distribution for stochastic policy.\n        if self.distribution:\n            actions = self.distribution(actions)\n\n        return actions","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:39:36.297858Z","iopub.execute_input":"2024-07-23T15:39:36.298301Z","iopub.status.idle":"2024-07-23T15:39:36.312042Z","shell.execute_reply.started":"2024-07-23T15:39:36.298271Z","shell.execute_reply":"2024-07-23T15:39:36.310073Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from tonic.torch import models, normalizers\nimport torch\n\n\ndef ppo_swimmer_model(\n        n_joints=5,\n        action_noise=0.1,\n        critic_sizes=(64, 64),\n        critic_activation=nn.Tanh,\n        **swimmer_kwargs,\n):\n    return models.ActorCritic(\n        actor=SwimmerActor(\n            swimmer=CE_NCAP_RS(n_joints=n_joints, **swimmer_kwargs),\n            distribution=lambda x: torch.distributions.normal.Normal(x, action_noise),\n        ),\n        critic=models.Critic(\n            encoder=models.ObservationEncoder(),\n            torso=models.MLP(critic_sizes, critic_activation),\n            head=models.ValueHead(),\n        ),\n        observation_normalizer=normalizers.MeanStd(),\n    )\n\n\ndef d4pg_swimmer_model(\n  n_joints=5,\n  critic_sizes=(256, 256),\n  critic_activation=nn.ReLU,\n  **swimmer_kwargs,\n):\n  return models.ActorCriticWithTargets(\n    actor=SwimmerActor(swimmer=CE_NCAP_RS(n_joints=n_joints, **swimmer_kwargs),),\n    critic=models.Critic(\n      encoder=models.ObservationActionEncoder(),\n      torso=models.MLP(critic_sizes, critic_activation),\n      # These values are for the control suite with 0.99 discount.\n      head=models.DistributionalValueHead(-150., 150., 51),\n    ),\n    observation_normalizer=normalizers.MeanStd(),\n  )","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:39:36.592861Z","iopub.execute_input":"2024-07-23T15:39:36.593549Z","iopub.status.idle":"2024-07-23T15:39:36.605097Z","shell.execute_reply.started":"2024-07-23T15:39:36.593515Z","shell.execute_reply":"2024-07-23T15:39:36.604159Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Verbose check true - yeah these are just scalars being passed around. My ability to do much with those is limited. Let me print out their values and see the range. If they're purely binary then not much can be done here. Otherwise, may be able to still have a linear layer map intelligently in and out of a shared matrix.","metadata":{}},{"cell_type":"code","source":"train('import tonic.torch',\n      # 'tonic.torch.agents.D4PG(model=d4pg_swimmer_model(n_joints=5,critic_sizes=(128,128)))',\n      'tonic.torch.agents.PPO(model=ppo_swimmer_model(n_joints=5,critic_sizes=(256,256),verbose=True))',\n  'tonic.environments.ControlSuite(\"swimmer-default_worm\",time_feature=True)',\n  name = 'CE_NCAP_RS_ppo',\n  trainer = 'tonic.Trainer(steps=int(1e5),save_steps=int(5e4))')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}